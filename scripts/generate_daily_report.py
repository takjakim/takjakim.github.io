#!/usr/bin/env python3
"""Generate daily market report JSON + Markdown.

Creates:
- assets/data/heatmaps/YYYY-MM-DD/daily.json
- _notes/investing/YYYY-MM-DD-daily.md

Data sources:
- Indices: Stooq for NDX/SPX/HSI; yfinance for CSI300 (000300.SS) fallback
- Items: Stooq per-ticker daily CSV for the 30-ticker watchlist
- News: web fetch (best-effort). If parsing fails, leaves empty arrays.

Designed to run locally (Clawdbot cron) after US close (KST 06:00+).
"""

from __future__ import annotations

import argparse
import datetime as dt
import json
import os
import re
from pathlib import Path

import requests

try:
    import yfinance as yf
except Exception:
    yf = None

ROOT = Path(__file__).resolve().parents[1]
UA = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X)"}

STOOQ_DAILY = "https://stooq.com/q/d/l/?s={sym}&i=d"

WATCHLIST = {
    "US": [
        ("aapl.us", "AAPL", "Apple", "Technology"),
        ("msft.us", "MSFT", "Microsoft", "Technology"),
        ("nvda.us", "NVDA", "NVIDIA", "Technology"),
        ("googl.us", "GOOGL", "Alphabet", "Technology"),
        ("amzn.us", "AMZN", "Amazon", "Consumer"),
        ("meta.us", "META", "Meta", "Technology"),
        ("tsla.us", "TSLA", "Tesla", "Automotive"),
        ("avgo.us", "AVGO", "Broadcom", "Semiconductor"),
        ("brk-b.us", "BRK-B", "Berkshire", "Financials"),
        ("jpm.us", "JPM", "JPMorgan", "Financials"),
        ("lly.us", "LLY", "Eli Lilly", "Healthcare"),
        ("v.us", "V", "Visa", "Financials"),
        ("unh.us", "UNH", "UnitedHealth", "Healthcare"),
        ("xom.us", "XOM", "ExxonMobil", "Energy"),
        ("ma.us", "MA", "Mastercard", "Financials"),
    ],
    "HK": [
        ("0700.hk", "0700.HK", "Tencent", "Technology"),
        ("9988.hk", "9988.HK", "Alibaba", "Technology"),
        ("0005.hk", "0005.HK", "HSBC", "Financials"),
        ("1299.hk", "1299.HK", "AIA", "Insurance"),
        ("0941.hk", "0941.HK", "China Mobile", "Telecom"),
        ("2318.hk", "2318.HK", "Ping An", "Insurance"),
        ("0388.hk", "0388.HK", "HKEX", "Financials"),
        ("0939.hk", "0939.HK", "CCB", "Financials"),
        ("1398.hk", "1398.HK", "ICBC", "Financials"),
        ("3690.hk", "3690.HK", "Meituan", "Technology"),
    ],
    "CN": [
        ("600519.cn", "600519.SS", "ê·€ì£¼ëª¨íƒœ", "Consumer"),
        ("601318.cn", "601318.SS", "í•‘ì•ˆë³´í—˜", "Insurance"),
        ("000858.cn", "000858.SZ", "ì˜¤ëŸ‰ì•¡", "Consumer"),
        ("600036.cn", "600036.SS", "ì´ˆìƒì€í–‰", "Financials"),
        ("601166.cn", "601166.SS", "í¥ì—…ì€í–‰", "Financials"),
    ],
}


def stooq_last_two_closes(sym: str, date: str | None = None):
    """Return (close, prev_close, effective_date) for given symbol.

    If date is given, tries to find that row; otherwise uses last row.
    """
    url = STOOQ_DAILY.format(sym=sym)
    txt = requests.get(url, headers=UA, timeout=30).text
    if "Exceeded the daily hits limit" in txt:
        raise RuntimeError("stooq_daily_hits_limit")
    lines = [ln for ln in txt.splitlines() if ln.strip()]
    if not lines or not lines[0].startswith("Date,"):
        raise RuntimeError(f"no_csv_for_{sym}")

    def parse_row(ln):
        parts = ln.split(",")
        return parts[0], float(parts[4])

    if date:
        # scan from end
        for i in range(len(lines) - 1, 0, -1):
            d, c = parse_row(lines[i])
            if d == date:
                dprev, cprev = parse_row(lines[i - 1])
                return c, cprev, d
        raise RuntimeError(f"date_not_found_{sym}_{date}")

    # last two rows
    d1, c1 = parse_row(lines[-1])
    d0, c0 = parse_row(lines[-2])
    return c1, c0, d1


def pct_change(close: float, prev: float) -> float:
    return (close - prev) / prev * 100.0 if prev else 0.0


def fetch_indices(target_date: str):
    """Fetch 4 indices. Prefer Stooq, fall back to yfinance when Stooq is rate-limited."""
    indices = {}

    mapping = {
        "NDX": "^ndx",
        "SPX": "^spx",
        "HSI": "^hsi",
    }

    stooq_ok = True
    for k, sym in mapping.items():
        try:
            close, prev, eff = stooq_last_two_closes(sym, date=target_date)
            indices[k] = {"close": round(close, 2), "pct": round(pct_change(close, prev), 2), "date": eff}
        except Exception as e:
            stooq_ok = False
            indices[k] = {"close": None, "pct": None, "date": None}

    # CSI300: Stooq unreliable; yfinance single-ticker
    if yf is not None:
        try:
            df = yf.download("000300.SS", start=(dt.date.fromisoformat(target_date) - dt.timedelta(days=7)).isoformat(), end=(dt.date.fromisoformat(target_date) + dt.timedelta(days=2)).isoformat(), interval="1d", progress=False, threads=False)
            if not df.empty:
                df.index = df.index.tz_localize(None)
                t = dt.datetime.fromisoformat(target_date)
                if t in df.index:
                    i = list(df.index).index(t)
                    if i > 0:
                        close = float(df["Close"].iloc[i])
                        prev = float(df["Close"].iloc[i - 1])
                        indices["CSI300"] = {"close": round(close, 2), "pct": round(pct_change(close, prev), 2), "date": target_date}
        except Exception:
            pass

    indices.setdefault("CSI300", {"close": None, "pct": None, "date": None})

    # If Stooq is rate-limited, try yfinance for US/HK indices too (few calls)
    if (not stooq_ok) and yf is not None:
        yf_map = {
            "NDX": "^NDX",
            "SPX": "^GSPC",
            "HSI": "^HSI",
        }
        for k, sym in yf_map.items():
            if indices.get(k, {}).get("close") is not None:
                continue
            try:
                df = yf.download(sym, start=(dt.date.fromisoformat(target_date) - dt.timedelta(days=7)).isoformat(), end=(dt.date.fromisoformat(target_date) + dt.timedelta(days=2)).isoformat(), interval="1d", progress=False, threads=False)
                if df.empty:
                    continue
                df.index = df.index.tz_localize(None)
                t = dt.datetime.fromisoformat(target_date)
                if t in df.index:
                    i = list(df.index).index(t)
                    if i > 0:
                        close = float(df["Close"].iloc[i])
                        prev = float(df["Close"].iloc[i - 1])
                        indices[k] = {"close": round(close, 2), "pct": round(pct_change(close, prev), 2), "date": target_date}
            except Exception:
                continue

    return indices


def fetch_items(target_date: str):
    items = []
    sess = requests.Session()

    for market, rows in WATCHLIST.items():
        for stooq_sym, ticker, name, sector in rows:
            try:
                url = STOOQ_DAILY.format(sym=stooq_sym)
                txt = sess.get(url, headers=UA, timeout=30).text
                if "Exceeded the daily hits limit" in txt:
                    raise RuntimeError("stooq_daily_hits_limit")
                lines = [ln for ln in txt.splitlines() if ln.strip()]
                if not lines or not lines[0].startswith("Date,"):
                    continue
                # find date row
                prev_close = None
                cur_close = None
                for i in range(len(lines) - 1, 0, -1):
                    parts = lines[i].split(",")
                    if parts[0] == target_date:
                        cur_close = float(parts[4])
                        prev_close = float(lines[i - 1].split(",")[4])
                        break
                if cur_close is None or prev_close is None:
                    continue
                p = pct_change(cur_close, prev_close)
                items.append({
                    "ticker": ticker,
                    "name": name,
                    "sector": sector,
                    "pct": round(p, 2),
                    "market": market,
                })
            except Exception:
                continue

    return items


def pick_top(items, n=10):
    gainers = sorted(items, key=lambda x: x.get("pct", 0), reverse=True)[:n]
    losers = sorted(items, key=lambda x: x.get("pct", 0))[:n]
    return gainers, losers


def fetch_news_best_effort():
    # We cannot use web_search (no Brave key). Best-effort scrape via readability is unreliable.
    # We'll leave placeholders to be filled later.
    return [], []


def make_insight(indices, items):
    # Very lightweight heuristic.
    ndx = indices.get("NDX", {}).get("pct")
    spx = indices.get("SPX", {}).get("pct")
    hsi = indices.get("HSI", {}).get("pct")
    csi = indices.get("CSI300", {}).get("pct")

    word = "ë³€ë™ì„±"
    if ndx is not None and spx is not None and abs(ndx - spx) >= 0.7:
        word = "ë””ì»¤í”Œë§"
    elif ndx is not None and ndx > 0.8 and spx is not None and spx > 0.5:
        word = "ë¦¬ìŠ¤í¬ì˜¨"
    elif ndx is not None and ndx < -0.8 and spx is not None and spx < -0.5:
        word = "ë¦¬ìŠ¤í¬ì˜¤í”„"

    # one-line summary
    summary = "ë¯¸êµ­ì€ í˜¼ì¡°, ì•„ì‹œì•„ëŠ” ì •ì±…/ìˆ˜ê¸‰ ì´ìŠˆì— ë”°ë¼ ë”°ë¡œ ë…¸ëŠ” íë¦„."
    if ndx is not None and spx is not None and ndx < 0 and spx < 0 and (hsi is not None and hsi > 0):
        summary = "ë¯¸êµ­ì€ ì¡°ì •, í™ì½©/ì¤‘êµ­ì€ ë°˜ë“± â€” í•˜ë£¨ ë§Œì— í‘œì •ì´ ë°”ë€ ì¥."

    return word, summary


def fmt_idx_row(label, close, pct):
    if close is None or pct is None:
        return f"| {label} | ì¡°íšŒ ì‹¤íŒ¨ | ì¡°íšŒ ì‹¤íŒ¨ |"
    sign = "+" if pct > 0 else ""
    return f"| {label} | {close:,.2f} | {sign}{pct:.2f}% |"


def write_outputs(date: str, indices, items, gainers, losers, us_news, asia_news):
    out_dir = ROOT / "assets" / "data" / "heatmaps" / date
    out_dir.mkdir(parents=True, exist_ok=True)

    daily = {
        "date": date,
        "indices": {
            "NDX": {"close": indices["NDX"]["close"], "pct": indices["NDX"]["pct"]},
            "SPX": {"close": indices["SPX"]["close"], "pct": indices["SPX"]["pct"]},
            "HSI": {"close": indices["HSI"]["close"], "pct": indices["HSI"]["pct"]},
            "CSI300": {"close": indices["CSI300"]["close"], "pct": indices["CSI300"]["pct"]},
        },
        "items": items,
        "top_gainers": [{k: x[k] for k in ("ticker", "name", "pct", "market")} for x in gainers],
        "top_losers": [{k: x[k] for k in ("ticker", "name", "pct", "market")} for x in losers],
        "news": {"us": us_news, "asia": asia_news},
    }

    (out_dir / "daily.json").write_text(json.dumps(daily, ensure_ascii=False, indent=2), encoding="utf-8")

    word, one_line = make_insight(indices, items)

    md_path = ROOT / "_notes" / "investing" / f"{date}-daily.md"

    def table_rows(items_list):
        lines = []
        for i, x in enumerate(items_list, 1):
            sign = "+" if x["pct"] > 0 else ""
            lines.append(f"| {i} | {x['ticker']} ({x['name']}) | {sign}{x['pct']:.2f}% | {x['market']} |")
        return "\n".join(lines)

    us_lines = "\n".join([f"{i+1}. **{n.get('title','(ì œëª© ì—†ìŒ)')}** - {n.get('summary','')} _(CNBC)_" for i, n in enumerate(us_news)]) or "- (ë‰´ìŠ¤ ìˆ˜ì§‘ ë¯¸ì„¤ì •)"
    asia_lines = "\n".join([f"{i+1}. **{n.get('title','(ì œëª© ì—†ìŒ)')}** - {n.get('summary','')} _(SCMP)_" for i, n in enumerate(asia_news)]) or "- (ë‰´ìŠ¤ ìˆ˜ì§‘ ë¯¸ì„¤ì •)"

    md = f"""---
title: \"{date} ì‹œì¥ ë¦¬í¬íŠ¸\"
last_modified_at: {date}
---

# {date} ì‹œì¥ ë¦¬í¬íŠ¸

## ğŸ“Œ í•œì¤„ ìš”ì•½
> {one_line}

---

## ğŸ“ˆ ì£¼ìš” ì§€ìˆ˜

| ì§€ìˆ˜ | ì¢…ê°€ | ë“±ë½ë¥  |
|------|------|--------|
{fmt_idx_row('ğŸ‡ºğŸ‡¸ NASDAQ 100', indices['NDX']['close'], indices['NDX']['pct'])}
{fmt_idx_row('ğŸ‡ºğŸ‡¸ S&P 500', indices['SPX']['close'], indices['SPX']['pct'])}
{fmt_idx_row('ğŸ‡­ğŸ‡° í•­ì…ì§€ìˆ˜', indices['HSI']['close'], indices['HSI']['pct'])}
{fmt_idx_row('ğŸ‡¨ğŸ‡³ CSI 300', indices['CSI300']['close'], indices['CSI300']['pct'])}

---

## ğŸš€ ìƒìŠ¹ Top 10

| ìˆœìœ„ | ì¢…ëª© | ë“±ë½ë¥  | ì‹œì¥ |
|------|------|--------|------|
{table_rows(gainers) if gainers else '| - | - | - | - |'}

---

## ğŸ“‰ í•˜ë½ Top 10

| ìˆœìœ„ | ì¢…ëª© | ë“±ë½ë¥  | ì‹œì¥ |
|------|------|--------|------|
{table_rows(losers) if losers else '| - | - | - | - |'}

---

## ğŸ—ºï¸ íˆíŠ¸ë§µ
<div class=\"market-heatmap\" data-file=\"daily\" data-as-of=\"{date}\"></div>

---

## ğŸ“° ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤

### ë¯¸êµ­
{us_lines}

### ì•„ì‹œì•„
{asia_lines}

---

## âœï¸ ì¸ì‚¬ì´íŠ¸

### ì˜¤ëŠ˜ ì‹œì¥ì„ í•œ ë‹¨ì–´ë¡œ? **{word}**
- (ì§§ì€ ì´ìœ ë¥¼ ë‚´ì¼ ë” ë‹¤ë“¬ì)

### ë‚´ì¼ ì£¼ëª© í¬ì¸íŠ¸
1. ê¸ˆë¦¬/ë‹¬ëŸ¬ ë°©í–¥
2. ë¹…í…Œí¬ ì‹¤ì /ê°€ì´ë˜ìŠ¤
3. ì¤‘í™”ê¶Œ ì •ì±…/ìˆ˜ê¸‰

---

## ğŸ”— ì—°ê²° ë…¸íŠ¸
- íˆ¬ì ì›ì¹™: [[ì§€ìˆ˜ íˆ¬ì ì‹œì‘í•˜ê¸°]]
- ë¶„ì„ ë„êµ¬: [[AI íˆ¬ì ë¶„ì„ í™œìš©í•˜ê¸°]]
"""

    md_path.write_text(md, encoding="utf-8")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--date", required=True, help="YYYY-MM-DD")
    args = ap.parse_args()

    date = args.date

    indices = fetch_indices(date)
    items = fetch_items(date)
    gainers, losers = pick_top(items, 10)
    us_news, asia_news = fetch_news_best_effort()

    write_outputs(date, indices, items, gainers, losers, us_news, asia_news)


if __name__ == "__main__":
    main()
