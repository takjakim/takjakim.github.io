---
title: MMLU
last_modified_at: '2026-02-15'
permalink: /theory/mmlu/
---

# MMLU

## 한 줄 요약
AI가 인간처럼 다양한 과목(수학, 역사, 법, 의학 등 57개)을 얼마나 잘 이해하는지 평가하는 종합 시험.

## 쉬운 설명
MMLU는 "Massive Multitask Language Understanding"의 약자로, 한국어로는 "대규모 다중과목 언어 이해"이다.

### 쉽게 비유하면 "AI의 수능"이다.

인간이 수능에서 국어, 수학, 영어, 과학, 사회를 보듯이, MMLU는 AI가 **57개 과목**을 모두 테스트한다.

### 어떤 과목들이 있나요?
- **STEM**: 수학, 물리, 화학, 생물, 컴퓨터과학
- **인문학**: 철학, 역사, 법학, 윤리
- **사회과학**: 경제학, 심리학, 사회학, 정치학
- **전문분야**: 의학, 회계, 경영
- 기타 등등...

### 문제 형식:
4지선다형 객관식
```
문제: 양자역학에서 불확정성 원리를 제안한 사람은?
A) 아인슈타인
B) 하이젠베르크  ← 정답
C) 슈뢰딩거
D) 보어
```

### 난이도:
- 초등학교 수준 문제부터
- 대학 전공 수준 문제까지
- 전문가 수준 문제까지

## 핵심 포인트
- **57개 과목**: 인간 지식의 거의 모든 영역 포괄
- **15,908개 문제**: 충분히 많은 문제로 신뢰성 확보
- **5-shot 평가**: AI에게 예시 5개를 먼저 보여준 후 문제 풀게 함
- **객관식**: 주관식보다 평가가 명확하고 공정함

## 관련 개념
- [[Fine-tuning]] - MMLU 점수 향상을 위해 Fine-tuning 수행
- [[HumanEval]] - 코드 생성 능력 평가 (MMLU는 지식 평가)
- [[KoBEST]] - 한국어 버전의 종합 평가
- [[GSM8K]] - 수학 추론 평가 (MMLU의 수학 부분과 유사)
- [[Perplexity (PPL)]] - 학습 중 성능 모니터링, MMLU는 최종 평가

## R4 연구에서의 역할
MMLU는 R4 연구의 **주요 성능 측정 도구** 중 하나이다.

### 왜 MMLU를 사용하나?
1. **종합적 평가**: 57개 과목 → 모델의 전반적 지식 수준 파악
2. **국제 표준**: 전 세계 AI 연구에서 사용하는 벤치마크
3. **추론 능력 평가**: 단순 암기가 아닌 이해와 추론 필요

### R4 연구의 MMLU 사용:
- **평가 주기**: 매 500 스텝마다 측정
- **평가 방법**: 5-shot (예시 5개 제공 후 평가)
- **기대 결과**:
  - Random Fine-tuning 대비 +2~4% 향상
  - Fixed E→H 대비 +1~2% 향상
  - Fixed H→E 대비 +3~5% 향상

### 실험 조건별 예상 MMLU 점수:
```
Random:           65%
Fixed E→H:        67%
Fixed H→E:        63%
ZPD-Adaptive:     69%  ← R4 제안 방법
```

### MMLU가 중요한 이유:
- **추론 태스크**: R4 연구는 "추론 중심 태스크에서 ZPD-Adaptive가 더 효과적"이라는 가설(H3b) 검증
- MMLU는 대표적인 **추론 태스크**이므로, ZPD-Adaptive의 효과가 크게 나타날 것으로 예상

## 더 알아보기
- Hendrycks, D., et al. (2021). Measuring Massive Multitask Language Understanding. *ICLR 2021*.
- 공개 데이터셋: https://github.com/hendrycks/test
- GPT-4 MMLU 점수: 86.4% (인간 전문가 수준)
- GPT-3.5 MMLU 점수: 70.0%
- 랜덤 찍기 기준선: 25% (4지선다)
- 인간 전문가 평균: ~89%
- 과목별 점수 차이가 큼: 쉬운 과목 90%+, 어려운 과목 50% 미만
